algorithm: PPO
total_timesteps: 1000000
n_envs: 4
checkpoint_freq: 50000
eval_freq: 10000
eval_episodes: 5

ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5

policy:
  net_arch:
    - 256
    - 256
  activation: tanh

environment:
  max_episode_steps: 1000
  opponent_type: static
